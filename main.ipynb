{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b402d9",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e591ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datapath = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804fabe",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d7211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(datapath + 'disaster_train.csv', sep=',',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b98eb",
   "metadata": {},
   "source": [
    "### Transfrom data\n",
    "\n",
    "we extract the text of the tweets as X vector and the transform them into a `tf-idf` (term-frequency times inverse document-frequency) matrix. \n",
    "\n",
    "__TODO__\n",
    "- At a later stage add location and keyword data to the feature matrix\n",
    "    - Problem: How do we addjust the weight of those, should they way as much as the occurence of one word or more?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24420e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 19097\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data['text']\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# the function TfidfVectorizer transforms our data in a sparse feature matrix. Every feature is the td-idf of a n-gram. The sizes of the \n",
    "# n-grams are set by our argument ngram_range.\n",
    "# with the argument stopwords we can remove words that typically occur often but do not give any information (i.e. and, a, ...)\n",
    "tf_computer = TfidfVectorizer(use_idf=False, ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "train_features = tf_computer.fit_transform(X_train)\n",
    "test_features = tf_computer.transform(X_test)\n",
    "\n",
    "print(\"vocabulary size:\", len(tf_computer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851a0462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.666125569461172"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "108206/19097"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6101c86",
   "metadata": {},
   "source": [
    "# First naive try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3fda2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73647059, 0.7452381 , 0.73079179])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select a linear model based on the assumption that specific words (linearly) imply a real disaster or not\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# TODO: vary hpyerparameter alpha\n",
    "\n",
    "clf = RidgeClassifier()\n",
    "scores = cross_val_score(clf, train_features, y_train, cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0436301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score: 0.945\n",
      "Test F1 score: 0.744\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train_features, y_train)\n",
    "y_train_pred = clf.predict(train_features)\n",
    "y_test_pred = clf.predict(test_features)\n",
    "\n",
    "\n",
    "print('Train F1 score:', np.round(f1_score(y_train, y_train_pred), 3))\n",
    "print('Test F1 score:', np.round(f1_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13355d1e",
   "metadata": {},
   "source": [
    "This looks very much like overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9127610",
   "metadata": {},
   "source": [
    "# Second (a bit less) naive try\n",
    "Lets try a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c747c59",
   "metadata": {},
   "source": [
    "Get some ideas [here](https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519d3ce",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082c094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-06 11:42:21.184179: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-06 11:42:21.184201: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow #the backend used by Keras (there are different beckend)\n",
    "from tensorflow.keras.models import Sequential #import the type of mpdel: sequential (e.g., MLP)\n",
    "from tensorflow.keras.layers import Dropout, Input, Dense #simple linear layer\n",
    "from tensorflow.keras.utils import to_categorical # transformation for classification labels\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.python.framework.random_seed import set_random_seed\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2540ddd1",
   "metadata": {},
   "source": [
    "### Create early stopping mechanism for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d441aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# simple early stopping\n",
    "es = EarlyStopping(monitor='val_loss', #quantity to be monitored\n",
    "                   mode='min', #we look for decreasing patterns stop \n",
    "                   patience = 3, #number of epochs with no improvement\n",
    "                   verbose=1)\n",
    "\n",
    "np.random.seed(123)\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c278300",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_den = train_features.todense()\n",
    "feature_vector_length = train_features_den.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e083f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "162/162 - 3s - loss: 0.5418 - accuracy: 0.7320 - val_loss: 0.4773 - val_accuracy: 0.7869 - 3s/epoch - 21ms/step\n",
      "Epoch 2/10\n",
      "162/162 - 3s - loss: 0.2105 - accuracy: 0.9196 - val_loss: 0.5895 - val_accuracy: 0.7730 - 3s/epoch - 16ms/step\n",
      "Epoch 3/10\n",
      "162/162 - 3s - loss: 0.0737 - accuracy: 0.9741 - val_loss: 0.7752 - val_accuracy: 0.7776 - 3s/epoch - 17ms/step\n",
      "Epoch 4/10\n",
      "162/162 - 3s - loss: 0.0375 - accuracy: 0.9876 - val_loss: 0.8302 - val_accuracy: 0.7668 - 3s/epoch - 17ms/step\n",
      "Epoch 5/10\n",
      "162/162 - 3s - loss: 0.0250 - accuracy: 0.9913 - val_loss: 0.9025 - val_accuracy: 0.7745 - 3s/epoch - 16ms/step\n",
      "Epoch 6/10\n",
      "162/162 - 3s - loss: 0.0192 - accuracy: 0.9925 - val_loss: 0.9345 - val_accuracy: 0.7629 - 3s/epoch - 17ms/step\n",
      "Epoch 7/10\n",
      "162/162 - 3s - loss: 0.0124 - accuracy: 0.9952 - val_loss: 0.9911 - val_accuracy: 0.7730 - 3s/epoch - 17ms/step\n",
      "Epoch 8/10\n",
      "162/162 - 3s - loss: 0.0112 - accuracy: 0.9952 - val_loss: 1.0185 - val_accuracy: 0.7676 - 3s/epoch - 16ms/step\n",
      "Epoch 9/10\n",
      "162/162 - 3s - loss: 0.0094 - accuracy: 0.9967 - val_loss: 1.0381 - val_accuracy: 0.7568 - 3s/epoch - 17ms/step\n",
      "Epoch 10/10\n",
      "162/162 - 3s - loss: 0.0095 - accuracy: 0.9961 - val_loss: 1.0508 - val_accuracy: 0.7622 - 3s/epoch - 17ms/step\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() #we first define how the \"model\" looks like\n",
    "model.add(Dense(input_dim = feature_vector_length, units=100 , activation='relu')) #input layer\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax')) #output layer\n",
    "\n",
    "# Configure the model and start training\n",
    "model.compile(loss='sparse_categorical_crossentropy', #loss metric\n",
    "    optimizer='adam',  #optimizer\n",
    "    metrics=['accuracy']) #displayed metric\n",
    "\n",
    "\n",
    "history = model.fit(train_features_den, y_train, epochs=10, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed1d9c1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25372/4104817030.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "# plt.ylim(0.8, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f525483f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25372/4106730074.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_den\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features_den\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(train_features_den, y_train)\n",
    "y_train_pred = clf.predict(train_features_den)\n",
    "y_test_pred = clf.predict(test_features.todense())\n",
    "\n",
    "\n",
    "print('Train F1 score:', np.round(f1_score(y_train, y_train_pred), 3))\n",
    "print('Test F1 score:', np.round(f1_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281e4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score: 0.954\n",
      "Test F1 score: 0.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Train F1 score:', np.round(accuracy_score(y_train, y_train_pred), 3))\n",
    "print('Test F1 score:', np.round(accuracy_score(y_test, y_test_pred), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002d76c",
   "metadata": {},
   "source": [
    "The former model does not improve after the first few epochs, afterwards train and validation acc both only slightly fluctuate. The Train accuracy raises quickly over the first epochs up to 99.7% (epoch 9). The validation accuracy on the other hand slightly falls. This indicates that our model not really learns a lot. It becomes able to predict the training set very well, but the accuracy on the validattion set stays at the same level (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482e917",
   "metadata": {},
   "source": [
    "### Model Variation\n",
    "\n",
    "In the following section we will try out different architectures for our model. To do so we will vary the activation function and the depth of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e70ce2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  the function can be used in a loop to define several models\n",
    "\n",
    "def MLP_definer(n_layers, n_features, n_classes, activation_f):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim = n_features, units=500, activation=activation_f))\n",
    "    for i in range(1, n_layers):\n",
    "        model.add(Dense(units=500-10*i^2, activation=activation_f))\n",
    "        \n",
    "    model.add(Dense(units=n_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation function: relu\n",
      "Epoch 00004: early stopping\n",
      "Number of layers: 2 \n",
      "Train Acc: 0.984957754611969 \n",
      "Test Acc:  0.7756489515304565\n"
     ]
    }
   ],
   "source": [
    "out = {}\n",
    "for fun in ('relu', 'sigmoid'):\n",
    "    print(f'Activation function: {fun}')\n",
    "    for i in range(2,7):    \n",
    "        model = MLP_definer(i, n_features=feature_vector_length, n_classes=2, activation_f=fun)\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics='accuracy'\n",
    "        )\n",
    "        history = model.fit(train_features_den, y_train, batch_size=16, epochs=500, validation_split=0.25, verbose=0, callbacks=[es])\n",
    "        out[i] = [history.history['accuracy'][-1],  history.history['val_accuracy'][-1]]\n",
    "        print(f'Number of layers: {i} \\nTrain Acc: {history.history[\"accuracy\"][-1]} \\nTest Acc:  {history.history[\"val_accuracy\"][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deadda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
